{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of extracting representation model digest\n",
    "\n",
    "This example is based on the readthedocs example for schnetpack library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "qm9tut = './qm9tut'\n",
    "if not os.path.exists('qm9tut'):\n",
    "    os.makedirs(qm9tut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm split.npz\n",
    "\n",
    "DB_PATH = \"./qm9.db\"\n",
    "PROPERTY = QM9.Cv\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN = 110000\n",
    "NUM_VALIDATION = 10000\n",
    "CUTOFF = 5.\n",
    "N_ATOM_BASIS = 32\n",
    "T = 3\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 1\n",
    "PIN_MEMORY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6875/6875 [01:07<00:00, 102.55it/s]\n"
     ]
    }
   ],
   "source": [
    "qm9data = QM9(\n",
    "    DB_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_train=NUM_TRAIN,\n",
    "    num_val=NUM_VALIDATION,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=float(CUTOFF)),\n",
    "        trn.RemoveOffsets(PROPERTY, remove_mean=True, remove_atomrefs=True),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    num_workers=NUM_WORKERS,\n",
    "    split_file=os.path.join(qm9tut, \"split.npz\"),\n",
    "    pin_memory=PIN_MEMORY, # set to false, when not using a GPU\n",
    "    load_properties=[PROPERTY], #only load U0 property\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=CUTOFF)\n",
    "schnet = spk.representation.SchNet(\n",
    "    n_atom_basis=N_ATOM_BASIS, n_interactions=T,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(CUTOFF)\n",
    ")\n",
    "pred_property = spk.atomistic.Atomwise(n_in=N_ATOM_BASIS, output_key=PROPERTY)\n",
    "\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=schnet,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_property],\n",
    "    postprocessors=[trn.CastTo64(), trn.AddOffsets(PROPERTY, add_mean=True, add_atomrefs=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_property = spk.task.ModelOutput(\n",
    "    name=PROPERTY,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aimas/dtu/dl/DeepLearningProject/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_property],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": LR}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Train a model with a few epochs, if you don't have a trained model already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 18.3 K\n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "18.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.3 K    Total params\n",
      "0.073     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aimas/dtu/dl/DeepLearningProject/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aimas/dtu/dl/DeepLearningProject/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/aimas/dtu/dl/DeepLearningProject/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 6875/6875 [09:19<00:00, 12.29it/s, v_num=1, val_loss=0.120]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 6875/6875 [09:19<00:00, 12.28it/s, v_num=1, val_loss=0.120]\n"
     ]
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir=qm9tut)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(qm9tut, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=qm9tut,\n",
    "    max_epochs=EPOCHS, # for testing, we restrict the number of epochs\n",
    ")\n",
    "trainer.fit(task, datamodule=qm9data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting digest\n",
    "\n",
    "The following code extracts the representation vector digest, following a forward pass of the model. This is done using Pytorch hooks via the `register_forward_hook`-method. \n",
    "First, we make a plot to get an overview of the model layers and their name references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessors.1.atomref torch.Size([100])\n",
      "postprocessors.1.mean torch.Size([])\n",
      "representation.radial_basis.widths torch.Size([20])\n",
      "representation.radial_basis.offsets torch.Size([20])\n",
      "representation.cutoff_fn.cutoff torch.Size([1])\n",
      "representation.embedding.weight torch.Size([100, 32])\n",
      "representation.interactions.0.in2f.weight torch.Size([32, 32])\n",
      "representation.interactions.0.f2out.0.weight torch.Size([32, 32])\n",
      "representation.interactions.0.f2out.0.bias torch.Size([32])\n",
      "representation.interactions.0.f2out.1.weight torch.Size([32, 32])\n",
      "representation.interactions.0.f2out.1.bias torch.Size([32])\n",
      "representation.interactions.0.filter_network.0.weight torch.Size([32, 20])\n",
      "representation.interactions.0.filter_network.0.bias torch.Size([32])\n",
      "representation.interactions.0.filter_network.1.weight torch.Size([32, 32])\n",
      "representation.interactions.0.filter_network.1.bias torch.Size([32])\n",
      "representation.interactions.1.in2f.weight torch.Size([32, 32])\n",
      "representation.interactions.1.f2out.0.weight torch.Size([32, 32])\n",
      "representation.interactions.1.f2out.0.bias torch.Size([32])\n",
      "representation.interactions.1.f2out.1.weight torch.Size([32, 32])\n",
      "representation.interactions.1.f2out.1.bias torch.Size([32])\n",
      "representation.interactions.1.filter_network.0.weight torch.Size([32, 20])\n",
      "representation.interactions.1.filter_network.0.bias torch.Size([32])\n",
      "representation.interactions.1.filter_network.1.weight torch.Size([32, 32])\n",
      "representation.interactions.1.filter_network.1.bias torch.Size([32])\n",
      "representation.interactions.2.in2f.weight torch.Size([32, 32])\n",
      "representation.interactions.2.f2out.0.weight torch.Size([32, 32])\n",
      "representation.interactions.2.f2out.0.bias torch.Size([32])\n",
      "representation.interactions.2.f2out.1.weight torch.Size([32, 32])\n",
      "representation.interactions.2.f2out.1.bias torch.Size([32])\n",
      "representation.interactions.2.filter_network.0.weight torch.Size([32, 20])\n",
      "representation.interactions.2.filter_network.0.bias torch.Size([32])\n",
      "representation.interactions.2.filter_network.1.weight torch.Size([32, 32])\n",
      "representation.interactions.2.filter_network.1.bias torch.Size([32])\n",
      "output_modules.0.outnet.0.weight torch.Size([16, 32])\n",
      "output_modules.0.outnet.0.bias torch.Size([16])\n",
      "output_modules.0.outnet.1.weight torch.Size([1, 16])\n",
      "output_modules.0.outnet.1.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trained_model = torch.load(os.path.join(qm9tut, 'best_inference_model'))\n",
    "\n",
    "for name, param in trained_model.state_dict().items():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a callback function that will be called by the hook. This function will be called with the following arguments:\n",
    "- `module`: the layer that the hook is attached to\n",
    "- `input`: the input digest to the layer\n",
    "- `output`: the output digest of the layer\n",
    "\n",
    "The function implemented below will take the input digest for the first property inference layer and append the digest Pytorch tensor to a static list `digest_inf_input`. \n",
    "\n",
    "**NOTE: Adding a hook to the output of the last representation layer should provide the same digest as for the input of the inference layer. Somehow they are not the same. By convention, we use a hook for the input of the infernece layer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of digests captured by forward pass hook: 1\n",
      "Number of digests captured by forward pass hook input: 1\n",
      "tensor([[-7.1627e-02, -2.9642e+00,  1.2790e-01,  6.6342e-02,  2.0104e+00,\n",
      "          8.2587e-04,  8.0413e-01, -6.5471e-01, -1.6550e+00, -2.2838e-01,\n",
      "         -1.1710e+00, -9.6679e-01, -2.0243e+00, -7.4430e-01, -7.2307e-01,\n",
      "         -1.5827e+00, -1.9718e+00, -3.1832e-01, -3.7698e-02,  7.7481e-01,\n",
      "         -2.3014e+00, -3.0325e-01,  2.5881e+00,  1.2734e+00,  9.6661e-01,\n",
      "         -1.0247e+00,  7.1486e-01,  1.6424e+00,  7.3638e-01, -3.6874e-01,\n",
      "         -1.0701e+00,  7.2358e-01],\n",
      "        [-1.1075e+00,  3.6289e-01,  1.6475e+00, -1.2829e-02, -9.0501e-01,\n",
      "         -2.8094e-01,  3.4885e-01,  1.7456e+00, -1.6498e+00, -4.7583e-01,\n",
      "          1.3172e+00,  5.0582e-01,  3.5705e-01,  2.3235e-01, -1.7651e-01,\n",
      "         -9.9803e-01,  8.6846e-01, -1.7836e+00,  3.2380e-01,  1.2094e+00,\n",
      "          3.4156e-01,  9.6595e-01, -8.1255e-01, -5.5443e-01,  1.4481e+00,\n",
      "          6.3974e-01,  3.9865e-01,  1.4886e-01,  7.8006e-02, -6.5559e-01,\n",
      "         -4.7991e-01,  4.0802e-01],\n",
      "        [-1.1075e+00,  3.6289e-01,  1.6475e+00, -1.2829e-02, -9.0501e-01,\n",
      "         -2.8094e-01,  3.4885e-01,  1.7456e+00, -1.6498e+00, -4.7583e-01,\n",
      "          1.3172e+00,  5.0583e-01,  3.5705e-01,  2.3235e-01, -1.7651e-01,\n",
      "         -9.9803e-01,  8.6845e-01, -1.7836e+00,  3.2380e-01,  1.2094e+00,\n",
      "          3.4156e-01,  9.6595e-01, -8.1255e-01, -5.5443e-01,  1.4481e+00,\n",
      "          6.3974e-01,  3.9865e-01,  1.4886e-01,  7.8007e-02, -6.5559e-01,\n",
      "         -4.7991e-01,  4.0802e-01],\n",
      "        [-1.1075e+00,  3.6288e-01,  1.6475e+00, -1.2835e-02, -9.0501e-01,\n",
      "         -2.8095e-01,  3.4883e-01,  1.7456e+00, -1.6497e+00, -4.7583e-01,\n",
      "          1.3173e+00,  5.0584e-01,  3.5705e-01,  2.3235e-01, -1.7652e-01,\n",
      "         -9.9804e-01,  8.6845e-01, -1.7836e+00,  3.2380e-01,  1.2094e+00,\n",
      "          3.4155e-01,  9.6594e-01, -8.1255e-01, -5.5442e-01,  1.4481e+00,\n",
      "          6.3974e-01,  3.9866e-01,  1.4886e-01,  7.8015e-02, -6.5560e-01,\n",
      "         -4.7992e-01,  4.0802e-01],\n",
      "        [-1.1075e+00,  3.6288e-01,  1.6475e+00, -1.2835e-02, -9.0501e-01,\n",
      "         -2.8095e-01,  3.4884e-01,  1.7456e+00, -1.6498e+00, -4.7583e-01,\n",
      "          1.3173e+00,  5.0584e-01,  3.5705e-01,  2.3235e-01, -1.7652e-01,\n",
      "         -9.9804e-01,  8.6845e-01, -1.7836e+00,  3.2380e-01,  1.2094e+00,\n",
      "          3.4155e-01,  9.6594e-01, -8.1255e-01, -5.5443e-01,  1.4481e+00,\n",
      "          6.3974e-01,  3.9866e-01,  1.4886e-01,  7.8014e-02, -6.5560e-01,\n",
      "         -4.7992e-01,  4.0802e-01]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([5, 32])\n",
      "tensor([[ 0.0105, -0.9848,  0.6436, -1.1284,  0.6879, -0.0699,  0.3629, -0.1696,\n",
      "          0.3672, -0.3580, -0.1539,  0.1883,  0.0591,  0.2829,  0.1565, -1.0503,\n",
      "          0.3351, -0.1756, -0.3679,  0.7642, -0.3012, -0.2465,  0.6822, -0.1188,\n",
      "          0.6539, -0.8431,  0.0408,  0.0350,  0.3430, -0.5349, -0.2148,  0.3967],\n",
      "        [ 0.1424, -0.0799,  0.3728, -0.0319, -0.1135,  0.0991,  0.1699,  0.1095,\n",
      "          0.1131,  0.1302,  0.1015, -0.2558, -0.0673,  0.1320,  0.1280,  0.1366,\n",
      "          0.2450,  0.2569,  0.0928, -0.0483,  0.0803,  0.0152, -0.1312, -0.1306,\n",
      "          0.2720, -0.1490, -0.0731,  0.1497,  0.0835,  0.1489, -0.0987,  0.0200],\n",
      "        [ 0.1424, -0.0799,  0.3728, -0.0319, -0.1135,  0.0991,  0.1699,  0.1095,\n",
      "          0.1131,  0.1302,  0.1015, -0.2558, -0.0673,  0.1320,  0.1280,  0.1366,\n",
      "          0.2450,  0.2569,  0.0928, -0.0483,  0.0803,  0.0152, -0.1312, -0.1306,\n",
      "          0.2720, -0.1490, -0.0731,  0.1497,  0.0835,  0.1489, -0.0987,  0.0200],\n",
      "        [ 0.1424, -0.0799,  0.3728, -0.0319, -0.1135,  0.0991,  0.1699,  0.1095,\n",
      "          0.1131,  0.1302,  0.1015, -0.2557, -0.0673,  0.1320,  0.1280,  0.1366,\n",
      "          0.2450,  0.2569,  0.0928, -0.0483,  0.0803,  0.0152, -0.1312, -0.1306,\n",
      "          0.2720, -0.1490, -0.0731,  0.1497,  0.0835,  0.1489, -0.0987,  0.0200],\n",
      "        [ 0.1424, -0.0799,  0.3728, -0.0319, -0.1135,  0.0991,  0.1699,  0.1095,\n",
      "          0.1131,  0.1302,  0.1015, -0.2558, -0.0673,  0.1320,  0.1280,  0.1366,\n",
      "          0.2450,  0.2569,  0.0928, -0.0483,  0.0803,  0.0152, -0.1312, -0.1306,\n",
      "          0.2720, -0.1490, -0.0731,  0.1497,  0.0835,  0.1489, -0.0987,  0.0200]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ase import Atoms\n",
    "\n",
    "digests_rep_output = []\n",
    "digest_inf_input = []\n",
    "converter = spk.interfaces.AtomsConverter(neighbor_list=trn.ASENeighborList(cutoff=float(CUTOFF)), \n",
    "                                          dtype=torch.float32,\n",
    "                                          device='cuda')\n",
    "\n",
    "def forward_hook_out_callback(module, input, output):\n",
    "    digests_rep_output.append(output)\n",
    "\n",
    "def forward_hook_in_callback(module, input, output):\n",
    "    digest_inf_input.append(input)\n",
    "\n",
    "# model_layer = trained_model.representation.interactions[2].f2out[1]\n",
    "model_layer_inf_input = trained_model.output_modules[0].outnet[0]\n",
    "# hook_handle = model_layer.register_forward_hook(forward_hook_out_callback)\n",
    "hook_handle_inf_input = model_layer_inf_input.register_forward_hook(forward_hook_in_callback)\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "numbers = np.array([6, 1, 1, 1, 1])\n",
    "positions = np.array([[-0.0126981359, 1.0858041578, 0.0080009958],\n",
    "                      [0.002150416, -0.0060313176, 0.0019761204],\n",
    "                      [1.0117308433, 1.4637511618, 0.0002765748],\n",
    "                      [-0.540815069, 1.4475266138, -0.8766437152],\n",
    "                      [-0.5238136345, 1.4379326443, 0.9063972942]])\n",
    "atoms = Atoms(numbers=numbers, positions=positions)\n",
    "inputs = converter(atoms)\n",
    "\n",
    "pred = trained_model(inputs)\n",
    "\n",
    "print(f\"Number of digests captured by forward pass hook: {len(digests_rep_output)}\")\n",
    "print(f\"Number of digests captured by forward pass hook input: {len(digest_inf_input)}\")\n",
    "\n",
    "for d in digest_inf_input:\n",
    "    print(d[0])\n",
    "    print(d[0].shape)\n",
    "\n",
    "for d in digests_rep_output:\n",
    "    print(d)\n",
    "    break\n",
    "\n",
    "# hook_handle.remove()\n",
    "hook_handle_inf_input.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = digests_rep_output[0].cpu().detach().numpy()\n",
    "y = digest_inf_input[0][0].cpu().detach().numpy()\n",
    "\n",
    "plt.scatter(\n",
    "    x[0],\n",
    "    y[0]\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
