#BSUB conventions:
The #BSUB lines are used to set options such as the queue, job name, number of cores, walltime limit, 
GPU resources, email notifications, output and error file paths, and other settings for the job. 
These comments help define how the job should be executed on the cluster. 
The job scheduler reads and interprets these comments to configure and schedule the job, and it's an 
established convention to use #BSUB for LSF job options.


Accessing the LSF cluster:
Guide for ssh access to the hpc here: https://www.hpc.dtu.dk/?page_id=2501. Easiest when on campus, 
but VPN can be used.
Command line access (on campus or VPN):

    ssh userid@login1.hpc.dtu.dk -> e.g., ssh s183700@login1.hpc.dtu.dk
    password: DTU password

    NOTE: Do not run any application here! Instead, switch to another node using:
    linuxsh


Adding the script and environment:
This should be done after loading the CUDA module, as it ensures that the Python script runs with the 
correct environment and dependencies. The CUDA module contains the NVIDIA CUDA Toolkit and related 
libraries and utilities. The CUDA Toolkit is a software development environment for building GPU-accelerated 
applications.
The script and environment can be added like so:

    source ./venv/bin/activate
    python ./your_script.py

    NOTE: If they are not located in the folder from where you run the .sh, the paths must be modified.


Running the .sh:
    1. Access the LSF cluster.
    2. Go to the folder where the environment and script are located.
    3. submit using: bsub < jobscript.sh